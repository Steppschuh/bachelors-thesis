\section{Related Work}
\label{sec:relatedwork}

\subsection{Motion Leaks through Smartwatch Sensors}
Wang et al. evaluated whether it is possible to figure out what a user is typing on a keyboard, just by looking at the sensor data that smartwatches produce.
In their paper ``MoLe: Motion Leaks through Smartwatch Sensors''\cite{paper:motionleaks}, they described how they analyzed peoples typing Behavior in order to identify possible information leaks when users type while wearing a smartwatch.
They also developed a system for the Samsung Gear Live smartwatch (one of the first devices powered by Android Wear) that resembles motion data to commonly used english words.

Although MoLe turned out to be able to detect typed words with an high accuracy, they used the wearable to record the sensor data only.
Files containing the data were exported from the watch and processed on powerful PCs and not on mobile devices, which does not meet our project's requirements.

\subsection{Real-Time Sensing on Android}
In ``Real-Time Sensing on Android''\cite{paper:realtimesensing}, Yin Yan et al. examined Android’s sensor architecture and whether it is suitable for use in a real-time context.
They took an in-depth look at the very low-level implementations of the Android |SensorManager|\cite{androiddocs:sensormanager}, including the kernel, HAL\footnote{Hardware Abstraction Layer}, and the |SensorService| (which polls raw sensor data through the HAL).
Their research showed that Android's sensor architecture does not provide predictable sensing.
It does not have any priority support in sensor data delivery, because all sensor data follows a single path from the kernel to apps.
Also, the amount of time it takes to deliver sensor data is unpredictable because Android relies heavily on polling and buffering.

For our project however, we worked around these limitations by implementing algorithms that abstracted from the data frequency and delay.

\clearpage

\subsection{Project Abacus}
Just like our project, Google wants to get rid of passwords using mobile devices.
The Advanced Technology and Projects group developed a product codenamed ``Project Abacus'', which is constantly paying attention to how a user is interacting with a mobile device.
It combines multiple factors, including how a users types, voice and face detection, and how apps are used.
The project was recently re-branded as the ``Trust API'', which provides third-party developers access to a score calculated by the system.

Although we have no doubt that Google is able to perform all required steps without drastically impacting the device's battery life, we can not except that data will be sent to Google servers.
This ultimately leads to privacy concerns.
In addition to that, there are no plans to support wearable devices or to make the API accessible for services which are not running on a user's mobile device.

\subsection{Recognizing ADLs in Real Time}
Waldhör Klemens and Rob Baldauf published their work about activity detection in ``Recognizing Drinking ADLs in Real Time using Smartwatches and Data Mining''\cite{paper:drinkingadls}.
They developed an app that is capable of detecting ADLs\footnote{Activities Of Daily Living} related to drinking.
Running on the Samsung Gear S (powered by Tizen OS), the app is collecting sensor data and repeatedly fitting models onto it in order to detect activities.
These models have previously been generated by applying data mining (logistic regression, neural networks, discriminant analysis, and random trees) on features extracted from recorded sample data. This way they achieved a classification accuracy of 92\% to 97\%, depending on the models used.

The authors' approach is very similar to ours, but they decided to execute all data processing directly on the wearable instead of outsourcing it to a mobile device.
Because of that, they ended up having huge issues with energy management. We avoided this by using the solution presented in the course of this work.

\clearpage